{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Vineet_Nandkishore_VAE.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "UFmrFyxLdvFV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from numpy import array\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from scipy.stats import norm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "import keras\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.models import Model\n",
        "from keras import metrics\n",
        "from keras import backend as K\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "np.random.seed(237)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "azPiv9O0eFWB",
        "colab_type": "code",
        "outputId": "dfdb7e35-efaf-4df6-ae74-983a00eef187",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "(x_train, Y_train), (x_test, Y_test) = mnist.load_data()\n",
        "print(x_train.shape)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(x_train, Y_train, test_size=0.30, random_state=42)\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X_test, y_test, test_size=0.33, random_state=42)\n",
        "\n",
        "# Partion 1: X_train, y_train\n",
        "# Partion 2: X1_train, y1_train\n",
        "# Partion 3: X1_test, y1_test\n",
        "y_train = to_categorical(y_train, num_classes = 10)\n",
        "print(y_train.shape)\n",
        "y1_train = to_categorical(y1_train, num_classes = 10)\n",
        "\n",
        "y1_test = to_categorical(y1_test, num_classes = 10)\n",
        "\n",
        "\n",
        "print(X1_test.shape)\n",
        "X_train = X_train.astype('float32') / 255.\n",
        "X_train = X_train.reshape(-1,28,28,1)\n",
        "\n",
        "X_test = X_test.astype('float32') / 255.\n",
        "X_test = X_test.reshape(-1,28,28,1)\n",
        "\n",
        "X1_train = X1_train.astype('float32') / 255.\n",
        "X1_train = X1_train.reshape(-1,28,28,1)\n",
        "\n",
        "X1_test = X1_test.astype('float32') / 255.\n",
        "X1_test = X1_test.reshape(-1,28,28,1)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(42000, 10)\n",
            "(5940, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "shbLxYCOukDf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L2ljRkTGfStC",
        "colab_type": "code",
        "outputId": "a29b0d9b-2fdc-401b-b9fe-5eed51b8e42e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 268
        }
      },
      "cell_type": "code",
      "source": [
        "plt.figure(1)\n",
        "plt.subplot(221)\n",
        "plt.imshow(X_train[13][:,:,0])\n",
        "\n",
        "plt.subplot(222)\n",
        "plt.imshow(X_train[690][:,:,0])\n",
        "\n",
        "plt.subplot(223)\n",
        "plt.imshow(X_train[2375][:,:,0])\n",
        "\n",
        "plt.subplot(224)\n",
        "plt.imshow(X_train[41000][:,:,0])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAToAAAD7CAYAAAD6gVj5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGGZJREFUeJzt3X2MVNX9x/E3rA3gVixELU9bpZWe\nNAFrMVEp4UGR8hBSbECx3eIDaiXaFPuoDa1VtAWfAq2iqWiigk9UqnYNQYsglqhIKYgm9aAtUGTR\n9VeKiAi6C78/dvbuORd2dnZ25s6dM59XQjhnzszcw/Llyz3nnntul8OHDyMiErKupe6AiEixKdGJ\nSPCU6EQkeEp0IhI8JToRCZ4SnYgE75h8P2iMmQ+cDRwGZllr1xesVyIlpNgOT15ndMaYUcAga+0w\n4HLgDwXtlUiJKLbDlO/QdQzwNIC19p9AL2NMzyzvP6xfqfkl2Sm2y/dXm/JNdH2AD5z6B5nXRMqd\nYjtAhboY0aVA3yOSNortAOSb6Orx/5frB+zqfHdESk6xHaB8E93zwFQAY8xQoN5a+1HBeiVSOort\nAHXJd/cSY8w8YCRwCLjGWvt6lrdrEjw9NBRrh2K7bLUZ23knug5SMKSHEl1hKbbTo83Y1p0RIhI8\nJToRCZ4SnYgET4lORIKnRCciwVOiE5HgKdGJSPDy3o+u0m3ZssWr33nnnVH5/vvvb7Pt2muvLW7H\nROQIOqMTkeAp0YlI8JToRCR4utc1i/3790flH/7wh15bXV2dV9+9e3dU7tLFv+VuyJAhUXnjxo2F\n7GI+dK9rYZVlbAdK97qKSOVSohOR4Gno6njuuee8+h133BGVV69enfWz7s8xPnQ97rjjovKkSZO8\ntsWLF3e4n52koWthlUVsVwgNXUWkcinRiUjwlOhEJHgVN0cXn0t79dVXo/rw4cO998bn2jryvbla\nu3YtAGeffXbOn+kkzdEVVkFi+1e/+lVUXrFihdf297//vRCHyNmnn37q1d9///2o/Mc//tFru+qq\nq6JyTU1NcTvWPs3RiUjlUqITkeBVxNB1z549UXncuHFRed26dRxzTOsGLvGfhTEmKtfW1nptJ510\nkld3hxcPPPBAzn2bPHkyAMuWLePAgQNeW/fu3XP+ng7Q0LWwChLbb7zxRlT+xje+4bXdfPPNUfnN\nN9/02nbtan229te//nUA5s+fz49//GMaGhqitnj8ZnPDDTd49U2bNkXl+L+Rb3/721H5qaeeyvkY\nRaKhq4hULiU6EQmeEp2IBC/IOTp3Tg78267c5SSNjY3eHN1XvvIV73MvvvhiVO7Ro4fXNmjQIK+e\nbfeSbFp+/k1NTd53APTu3Tvn7+kAzdEVVs6xvXnz5qjcq1cv/0ucf4cDBw702g4dOhSV+/bt67W9\n9957Ubkl7pqamqiqqsq65CnXtnh7vK1Pnz5Rub6+nhJrM7Zz2krdGDMYeAaYb6292xhTAywGqoBd\nwHRr7cFC9FQkSYrtytDu0NUYUw3cBbzgvDwHWGitHQG8A8woTvdEikexXTlyOaM7CEwErnNeGw3M\nzJTrgJ8B9xa0Zx3knlI/8sgjXps7XI1zT73/9re/eW3uEpJbb73Va/vf//7n1d1h5rJly7y2gwdb\nTwgmTJjQZl/iy1J+/vOft/leKYhEY9uNmT/96U9e24knnhiV40PJK6+8MirffvvtXtvevXuPeqxt\n27Z5UzhLly5ts1+nnXaaV4/fpfP0009H5fjDnToyTVNK7SY6a20j0OiuKQOqndP5BqDvER8USTnF\nduXI+WKEMeZG4P8y8xgN1tqTMq+fCjxsrf1mlo+n5l5X0cWIOMV2MDp3MeIo9hljelhrPwH6AyW/\n3OIm7HvuucdrmzVr1lE/09jYyIABA6L6P/7xD68929B19uzZXt29itaRoat71TU+LNHQtSSKFttr\n1qyJyitXrvTa3KHrT37yE69txozWacJchq41NTXs2LEjkaGrO/Wzc+fONo9RavkmupXAFGBJ5vcV\n2d9efOvWrYvKbSU28HeJAHjttdeicvy2LtfQoUOzHv/ll1+OyvGlJ//973+zfrZF/PYeKYmixfao\nUaOOWgZ/6Ul8lHXmmWdG5eOPP95ri9db1NTUeLuJuA9o6ih32VW8b24STrN2E50x5gzgTuAU4DNj\nzFSgFnjQGHMVsB14qJidFCkGxXblyOVixAaar0TFjS14b0QSpNiuHPkOXUsuPhyMb5rpOvXUU6Py\n1Vdf7bX169cvp+ONHevHfmNjY06fi4uf+rur3pcsWeK1/fSnP43K8XkUCYs7hxZfsvHoo49G5Suu\nuCKxPh1NuSwnidO9riISPCU6EQmeEp2IBK9s5+i2bdvm1bPNHbjzGtmWkCQh3s+uXVv/r4nvTBHf\n4ULC5c4Bz507t4Q9yS6h3Y4KTmd0IhI8JToRCV7ZDl0XLFjQZtull17q1eO3raTVmDFjvHoKnpMp\nCfnyl78clePTG++++25U3r9/v9d27LHHFrdjMVpeIiKSUkp0IhI8JToRCV5ZzdG9//77Udl9YHRc\n/OEin/vc54rWp1zU1dWV9PiSft26dYvK8Xk3dyuxpOfk4rS8REQkpZToRCR4SnQiEryymqNzH9a7\nZcuWNt933nnnJdGdNsUfoL1w4cKoHJ/jcLetvvfekj5ITUrIvTXxX//6l9e2devWnL6joaEh+q6G\nhgbvc/F562xz3HF//etfo3J8HZ27K/azzz7rtXXt2pWJEyeyfPnyI57Mt2jRoqicxLyjzuhEJHhK\ndCISvJwfd9hJBTnIf/7zn6j8zW/6T6Bzh7XxpxF98YtfLMThc+buCAtw8cUXt/neG2+8ETjyoT1F\nVJ738KRXatZb1NbWAs0PcK+treWxxx6L2uJDTvfffba2eHu2Nne3bGgeujY1NVFVVXVE2+OPPx6V\np02b1vYfqmPajG2d0YlI8JToRCR4SnQiEryyWl7ypS99KSqfc845Xps7HxF/mvlvfvObqHzccccV\npW8HDhyIyjfddFPOnyv1jscSDnfJxqJFi7ylS+eff773XncJVHyOLv5Q7JtvvjkqP/DAA16b+9n4\nPHPL3Nvrr79+xHf27Nmz7T9IEeiMTkSCp0QnIsErq+UlLne1NsCECRPafO+gQYOi8po1a6JyIYeN\ny5Yti8oduVye74OwO0HLSworNctLimXdunVRediwYV6bO3T96KOPvLYS7LSi5SUiUrlyuhhhjLkN\nGJF5/1xgPbAYqAJ2AdOttQeL1UmRYlBcV452z+iMMecAg621w4DxwAJgDrDQWjsCeAeYUdReihSY\n4rqy5HJG9xLwWqa8B6gGRgMzM6/VAT8DEt16Y/jw4V7dvZT+yiuveG3uTifuQ6Jbbk9p8cQTT3if\n68gTj6ZOnRqV3YdSA/Tu3Tsqv/zyyzl/pxRVKuM6jc4666yoPHToUK9t06ZNUfnFF1/02iZOnFjU\nfnVEu4nOWtsEfJypXg4sB8Y5p/QNQN+jfVYkrRTXlSXnBcPGmMk0B8S3gLedppJcxYtf0Vm7dm1e\n39PU1FSI7pTtXvqVLm1xnXYd2ccuTXK9GDEOmA2Mt9Z+aIzZZ4zpYa39BOgP1Bezk7lwH+x77rnn\nem1t/eU0NjZyzDGtP4JsOzO0J9tuEEOGDInKGzduzPk7pbjKIa7TZseOHV79lFNOicqjRo3y2lat\nWpVEl3KSy8WI44HbgUnW2t2Zl1cCUzLlKcCK4nRPpDgU15UllzO6acAJwFJjTMtrlwD3G2OuArYD\nDxWneyJFo7iuILlcjLgPuO8oTWML3x2RZCiuK0tZ7V6SjXtx4rnnnvPa3J2JFyxY4LW582ebN28u\nSF/iO6R87WtfK8j3iqSNOx8dnwv/4IMPOPHEE6PfS0m3gIlI8JToRCR4Zbt7STH8+c9/9uruUPaW\nW27J+tnJkydH5dmzZ3tt8dXkJab1YYVVFrFdKNmWl4wcOdJrW716dRJdcmn3EhGpXEp0IhI8JToR\nCZ7m6CqP5ugKq6JiOz5HN3DgwKgcn6MrwS1gmqMTkcqlRCciwQvmzggRSZ479ZXmrcp0RiciwVOi\nE5HgKdGJSPA0RyciOevZs6dX79OnT1T+/ve/n3R3cqYzOhEJnhKdiARPd0ZUHt0ZUViK7fTQnREi\nUrmU6EQkeEp0IhK8pJaXaF5IQqXYLgM6oxOR4CnRiUjwlOhEJHhKdCISPCU6EQmeEp2IBC+x3UuM\nMfOBs2m+ZWaWtXZ9Usd2+jAYeAaYb6292xhTAywGqoBdwHRr7cGE+nIbMILmv4O5wPpS9UXyp7g+\noi+pjOtEzuiMMaOAQdbaYcDlwB+SOG6sD9XAXcALzstzgIXW2hHAO8CMhPpyDjA48/MYDywoVV8k\nf4rrI/qS2rhOaug6BngawFr7T6CXMaZn9o8U3EFgIlDvvDYa+EumXAecl1BfXgIuyJT3ANUl7Ivk\nT3HtS21cJzV07QNscOofZF7bm9DxsdY2Ao3GGPflauc0ugHom1BfmoCPM9XLgeXAuFL0RTpFce33\nJbVxXaodhtN420zifTLGTKY5IL4FvF3KvkhBpPHvTXFNckPXepr/p2vRj+aJyVLbZ4zpkSn3xz/9\nLypjzDhgNjDBWvthKfsieVNcx6Q1rpNKdM8DUwGMMUOBemvtRwkdO5uVwJRMeQqwIomDGmOOB24H\nJllrd5eyL9IpimtHmuM6qR2GMcbMA0YCh4BrrLWvJ3Lg1uOfAdwJnAJ8BuwEaoEHge7AduAya+1n\nCfTlB8CNwBbn5UuA+5Pui3SO4trrS2rjOrFEJyJSKrozQkSCp0QnIsHLe3lJGm59ESkGxXZ48jqj\nS8OtLyLFoNgOU75D147e+nJYv1LzS7JTbJfvrzblm+j60Hy7S4uWW19Eyp1iO0CFuhiRxltfRApB\nsR2AfBNdWm99EeksxXaA8k10ab31RaSzFNsByvvOiA7e+qJJ8PTQUKwdiu2y1WZsJ3ULmIIhPZTo\nCkuxnR5txrbujBCR4CnRiUjwlOhEJHhKdCISPCU6EQmeEp2IBE+JTkSCp0QnIsFTohOR4CnRiUjw\nlOhEJHh5PzNCRCrDxx9/HJXnzZvntT3xxBNReffu3V7b+vXrGThwIFu3bmXgwIHF7WQ7dEYnIsFT\nohOR4GnoKiKeTZs2efUZM2ZE5Z07d3pt119/fVQ+7bTTvLY+ffp4v5eSzuhEJHhKdCISPCU6EQle\nMHN0Xbq07qLctaufv4cMGRKVL7rooqh8/fXXH3G5PF+PP/54VH7jjTe8tt/97ndR+brrrivI8UQK\npa6uzqtfeOGFXn3w4MFRecOGDV7bgAEDovJ7773ntfXo0cP7vZR0RiciwVOiE5HgBfMUsHPPPTcq\nv/TSSzl9prGxkaqqqqjuDn/bE/+5Zfvsww8/HJW/973v5XyMItFTwAqrLJ8C9u9//zsqx5eFuENV\ngBdeeCEqV1dXe21vvfVWVP7Od77jtW3cuJHu3btz4MABunfv3uk+50BPARORyqVEJyLBU6ITkeAF\ns7zk+eefj8o7duzw2h599NGoXF9f77XNnDkzp++/5pprvPonn3zi1c8666w2Pztp0qScjiFSLG+/\n/bZXHz16dFQ+/fTTvbZHHnnEq8fn5Vzucq0vfOELXlvLvFxC83NZ5ZTojDGDgWeA+dbau40xNcBi\noArYBUy31h4sXjdFikOxXRnaHboaY6qBu4AXnJfnAAuttSOAd4AZR/usSJoptitHLmd0B4GJgLuk\nfzTQMuarA34G3FvQnnXQMce0/lHim/zNnj27zc8tXLgwr+Pdc889eX1OUqUsYjtf+/bti8qXXXaZ\n19bY2BiVFy9e7LWdfPLJbX5nfFrIWhuV40PgNGk30VlrG4FGY4z7crVzOt8A9C1C30SKSrFdOQpx\nMaIiF6BeffXVWesShLKO7c9//vNRee3atQX5zpqaGq8evyiXVvkmun3GmB7W2k+A/kB9ex8ITXzo\n+qMf/ajN97p76ffs2bNofZKCCCa23aHr+PHjvTb3Kuyrr77qtWV7vkN86PrVr341KseHrq+88kru\nnS2yfBPdSmAKsCTz+4qC9ahM7N2716u7t4QNHz7ca+vWrVsifZKCCCa2586dG5XXrVvntblLrjry\n4Jo333zTq3/66adRuba2tqNdTEy7ic4YcwZwJ3AK8JkxZipQCzxojLkK2A48VMxOihSDYrty5HIx\nYgPNV6Lixha8NyIJUmxXjmDujCg1d/cSd6NP0NBVkrFnzx6g+Q6FPXv2cO+9ratirrzySu+9F1xw\nQc7f684xX3vttV5b376tF6Xdh+ikje51FZHgKdGJSPCU6EQkeJqjy5M7/xF36aWXJtcRkYwnn3wS\ngCuuuIInn3ySDz/8MGqbPn16zt/jfg7gzDPPjMrbt2/32p566qmofOyxx3aov0nSGZ2IBE+JTkSC\np6Frnt59912v7i4vOeGEE5LujgirV68Gmoeuq1evZuTIkVFbtp1F4kPVX/7yl15969atUfn888/3\n2splU1md0YlI8JToRCR4SnQiErxgHmBdbI899phXj+/U4G6Ds3z58kT6lKey3mMthUoa25s3b47K\nLfNwhw4domvXrsybNy9q+8UvfuF9zn3I+yWXXOK1bdu2zau788/xbZr69++fX8eLQw+wFpHKpUQn\nIsFTohOR4GkdXY7ceYqj1bt21f8Zkjz3aV5uTHbp0oWHHmrdM3TOnDne5w4ebH1U7aFDh7y2eGxP\nmzYtKvfr169zHS4R/esUkeAp0YlI8DR0FSlj7q1ds2bN8sq///3vo3r8gU3ukpLf/va3Xtv+/fu9\n+h133BGV48PacqEzOhEJnhKdiARPiU5Egqc5OpEy5i5rch9YPXfuXG666aao3qNHD+9zS5cujcrx\nXYPjO2Sn7DavvOiMTkSCp0QnIsHT0DVH8V1e4vX46nKRpLkPSu/WrZtXj+8iPHPmzKjcq1cvr+2G\nG24oUg9LJ6dEZ4y5DRiRef9cYD2wGKgCdgHTrbUH2/4GkfRRXFeOdoeuxphzgMHW2mHAeGABMAdY\naK0dAbwDzChqL0UKTHFdWXKZo3sJuCBT3gNUA6OBv2ReqwPOK3jPRIpLcV1B2h26WmubgI8z1cuB\n5cA455S+AehbnO6lh/ugXtDuJeWu0uL6mWee8erubV733Xef13byyScn0qck5XwxwhgzmeaA+Bbw\nttNUnje/dZC77kjCUSlxffHFF2ethy7XixHjgNnAeGvth8aYfcaYHtbaT4D+QH0xO5kGF154oVdf\ntmyZV58wYUJUfvbZZxPpk3ROJcX1ww8/7NVnzGidflyyZInXdtFFFyXSpyS1m+iMMccDtwPnWWt3\nZ15eCUwBlmR+X1G0HqbEhg0bSt0FKaBKiGt3eHrrrbd6be5uJiEmtrhczuimAScAS40xLa9dAtxv\njLkK2A481MZnRdJKcV1BcrkYcR9w31Gaxha+OyLJUFxXFl0qFJHg6RYwkUC5KwXeeustr23RokVJ\nd6ekdEYnIsFTohOR4GnomqNbbrnFq9fW1np17V4ipdayo06XLl04fPiwt3aud+/e3nu/+93vJtq3\nUtMZnYgET4lORIKnRCciwdMcXY7GjvXXkWr3EkmbVatWATBmzBhWrVrFmjVrorZf//rX3nvjD8sJ\nnf51ikjwlOhEJHhd4g95KZJEDpKk008/3asPGDAgKqd8m6ag9llLgeBiu4y1Gds6oxOR4CnRiUjw\nlOhEJHhaXpKn+K6sa9euLVFPRKQ9OqMTkeAp0YlI8LS8pPJoeUlhKbbTQ8tLRKRyKdGJSPCU6EQk\neEktL9G8kIRKsV0GdEYnIsFTohOR4CnRiUjwlOhEJHhKdCISPCU6EQleYruXGGPmA2fTfMvMLGvt\n+qSO7fRhMPAMMN9ae7cxpgZYDFQBu4Dp1tqDCfXlNmAEzX8Hc4H1peqL5E9xfURfUhnXiZzRGWNG\nAYOstcOAy4E/JHHcWB+qgbuAF5yX5wALrbUjgHeAGQn15RxgcObnMR5YUKq+SP4U10f0JbVxndTQ\ndQzwNIC19p9AL2NMz4SO3eIgMBGod14bDfwlU64DzkuoLy8BF2TKe4DqEvZF8qe49qU2rpMauvYB\nNjj1DzKv7U3o+FhrG4FGY4z7crVzGt0A9E2oL03Ax5nq5cByYFwp+iKdorj2+5LauC7VDsNpvG0m\n8T4ZYybTHBDfAt4uZV+kINL496a4Jrmhaz3N/9O16EfzxGSp7TPGtDyyvD/+6X9RGWPGAbOBCdba\nD0vZF8mb4jomrXGdVKJ7HpgKYIwZCtRbaz9K6NjZrASmZMpTgBVJHNQYczxwOzDJWru7lH2RTlFc\nO9Ic10ntMIwxZh4wEjgEXGOtfT2RA7ce/wzgTuAU4DNgJ1ALPAh0B7YDl1lrP0ugLz8AbgS2OC9f\nAtyfdF+kcxTXXl9SG9eJJToRkVLRnREiEjwlOhEJnhKdiARPiU5EgqdEJyLBU6ITkeAp0YlI8JTo\nRCR4/w929Sypxy3iMAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 4 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "G9H6iwphfcBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "img_shape = (28, 28, 1)    # for MNIST\n",
        "batch_size = 16\n",
        "latent_dim = 32  # Number of latent dimension parameters\n",
        "\n",
        "# Encoder architecture: Input -> Conv2D*4 -> Flatten -> Dense\n",
        "input_img = keras.Input(shape=img_shape)\n",
        "\n",
        "x = layers.Conv2D(32, 3, padding='same', activation='relu')(input_img)\n",
        "x = layers.Conv2D(64, 3, padding='same', activation='relu', strides=(2, 2))(x)\n",
        "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "x = layers.Conv2D(64, 3, padding='same', activation='relu')(x)\n",
        "\n",
        "# need to know the shape of the network here for the decoder\n",
        "shape_before_flattening = K.int_shape(x)\n",
        "\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(latent_dim, activation='relu')(x)\n",
        "\n",
        "\n",
        "\n",
        "# Two outputs, latent mean and (log)variance\n",
        "z_mu = layers.Dense(latent_dim)(x)\n",
        "z_log_sigma = layers.Dense(latent_dim)(x)\n",
        "\n",
        "encoder = Model(input_img, x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pD7UetEEfi3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# sampling function\n",
        "def sampling(args):\n",
        "    z_mu, z_log_sigma = args\n",
        "    epsilon = K.random_normal(shape=(K.shape(z_mu)[0], latent_dim), mean=0., stddev=1.)\n",
        "    return z_mu + K.exp(z_log_sigma) * epsilon\n",
        "\n",
        "# sample vector from the latent distribution\n",
        "z = layers.Lambda(sampling)([z_mu, z_log_sigma])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j4MDLAXKfmLy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# decoder takes the latent distribution sample as input\n",
        "decoder_input = layers.Input(K.int_shape(z)[1:])\n",
        "\n",
        "# Expand to 784 total pixels\n",
        "x = layers.Dense(np.prod(shape_before_flattening[1:]),\n",
        "                 activation='relu')(decoder_input)\n",
        "\n",
        "x = layers.Reshape(shape_before_flattening[1:])(x)\n",
        "\n",
        "x = layers.Conv2DTranspose(32, 3, padding='same', activation='relu', strides=(2, 2))(x)\n",
        "x = layers.Conv2D(1, 3, padding='same', activation='sigmoid')(x)\n",
        "\n",
        "# decoder model statement\n",
        "decoder = Model(decoder_input, x)\n",
        "\n",
        "# apply the decoder to the sample from the latent distribution\n",
        "z_decoded = decoder(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0v1JDqfAfpQY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# construct a custom layer to calculate the loss\n",
        "class CustomVariationalLayer(keras.layers.Layer):\n",
        "\n",
        "    def vae_loss(self, x, z_decoded):\n",
        "        x = K.flatten(x)\n",
        "        z_decoded = K.flatten(z_decoded)\n",
        "        # Reconstruction loss\n",
        "        xent_loss = keras.metrics.binary_crossentropy(x, z_decoded)\n",
        "        # KL divergence\n",
        "        kl_loss = -5e-4 * K.mean(1 + z_log_sigma - K.square(z_mu) - K.exp(z_log_sigma), axis=-1)\n",
        "        return K.mean(xent_loss + kl_loss)\n",
        "\n",
        "    # adds the custom loss to the class\n",
        "    def call(self, inputs):\n",
        "        x = inputs[0]\n",
        "        z_decoded = inputs[1]\n",
        "        loss = self.vae_loss(x, z_decoded)\n",
        "        self.add_loss(loss, inputs=inputs)\n",
        "        return x\n",
        "\n",
        "# apply the custom loss to the input images and the decoded latent distribution sample\n",
        "y = CustomVariationalLayer()([input_img, z_decoded])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2kQrWKpkfuXb",
        "colab_type": "code",
        "outputId": "08b3d31f-fd9a-4580-ecc0-54366a294762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        }
      },
      "cell_type": "code",
      "source": [
        "# VAE model statement\n",
        "vae = Model(input_img, y)\n",
        "vae.compile(optimizer='rmsprop', loss=None, metrics = ['accuracy'])\n",
        "vae.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 28, 28, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 28, 28, 32)   320         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 14, 14, 64)   18496       conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 14, 14, 64)   36928       conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 14, 14, 64)   36928       conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 12544)        0           conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 32)           401440      flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 32)           1056        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 32)           1056        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 32)           0           dense_2[0][0]                    \n",
            "                                                                 dense_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "model_2 (Model)                 (None, 28, 28, 1)    432705      lambda_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "custom_variational_layer_1 (Cus [(None, 28, 28, 1),  0           input_1[0][0]                    \n",
            "                                                                 model_2[1][0]                    \n",
            "==================================================================================================\n",
            "Total params: 928,929\n",
            "Trainable params: 928,929\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tGnMK8GZf0tp",
        "colab_type": "code",
        "outputId": "5215b940-6d94-4686-81c6-b2e530d49469",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        }
      },
      "cell_type": "code",
      "source": [
        "vae.fit(x=X_train, y=None, shuffle=True, epochs=5, batch_size=batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "42000/42000 [==============================] - 37s 892us/step - loss: 0.1446\n",
            "Epoch 2/5\n",
            "42000/42000 [==============================] - 37s 871us/step - loss: 0.1032\n",
            "Epoch 3/5\n",
            "42000/42000 [==============================] - 37s 871us/step - loss: 0.0981\n",
            "Epoch 4/5\n",
            "42000/42000 [==============================] - 36s 860us/step - loss: 0.0956\n",
            "Epoch 5/5\n",
            "42000/42000 [==============================] - 36s 851us/step - loss: 0.0940\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe740216c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "metadata": {
        "id": "Dsw-4dTsPE5q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Loss goes to 0.0983 with 4 CNN and 3 Dense Layers that form the encoder and decoder. The accuracy increases significantly with the addition of more Conv layers to the architecture.\n",
        "**"
      ]
    },
    {
      "metadata": {
        "id": "eEN2upZz9TUK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "**PART 2**\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "NeC0jN48ulRJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "encoded_img = encoder.predict(X1_train) # Input to the next layer\n",
        "\n",
        "encoded_test= encoder.predict(X1_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cn8VWoQM3Gcg",
        "colab_type": "code",
        "outputId": "474779e9-388e-49bf-8a44-fc7fd3a43ae4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "print(X1_train.shape)\n",
        "print(array(encoded_img).shape)\n",
        "\n",
        "print(y1_train.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12060, 28, 28, 1)\n",
            "(12060, 32)\n",
            "(12060, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xiG5cjKL5x17",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "Out_img = keras.Input((latent_dim,))\n",
        "#Out = layers.Dense(64, activation='softmax')(Out_img)\n",
        "Out = layers.Dense(10, activation='softmax')(Out_img)\n",
        "\n",
        "Part2 = Model(Out_img, Out)\n",
        "Part2.summary()\n",
        "Part2.compile(optimizer='rmsprop', loss=\"categorical_crossentropy\", metrics = ['accuracy'])\n",
        "Part2.fit(x=encoded_img, y=y1_train, batch_size= 64, epochs = 50)#, validation_data=(X1_test, y1_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-gag2gVBJWnS",
        "colab_type": "code",
        "outputId": "ba46448e-c4fa-460e-c566-306f6456c6c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "scores = Part2.evaluate(encoded_test, y1_test, verbose=0)\n",
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.24464846319622463, 0.9314814814012059]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "8y7xAmAmPmlO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2) Accuracy goes to 94% Testing in part 2 with One Input Layer, one hidden layer and one Output Layer\n",
        " The model gives around the same accuracy, 93% with just one input and one output layer.**"
      ]
    },
    {
      "metadata": {
        "id": "2TmWMuTbu_SG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', activation ='relu', input_shape = (28,28,1)))\n",
        "\n",
        "model2.add(MaxPool2D(pool_size=(2,2)))\n",
        "model2.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "# model2.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n",
        "#                  activation ='relu'))\n",
        "# model2.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\n",
        "# model2.add(Dropout(0.25))\n",
        "\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(256, activation = \"relu\"))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(10, activation = \"softmax\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JUrQrrhLvIi6",
        "colab_type": "code",
        "outputId": "ca2f22cc-2d25-4ca2-efd2-a694a99bda2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        }
      },
      "cell_type": "code",
      "source": [
        "optimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "model2.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model2.summary()\n",
        "model2.fit(x=X_train, y=y_train, batch_size= 64, epochs = 10)#, validation_data=(X1_test, y1_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 28, 28, 32)        832       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 14, 14, 32)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 6272)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 256)               1605888   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                2570      \n",
            "=================================================================\n",
            "Total params: 1,609,290\n",
            "Trainable params: 1,609,290\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "42000/42000 [==============================] - 7s 157us/step - loss: 0.2292 - acc: 0.9300\n",
            "Epoch 2/10\n",
            "42000/42000 [==============================] - 6s 149us/step - loss: 0.0850 - acc: 0.9742\n",
            "Epoch 3/10\n",
            "42000/42000 [==============================] - 6s 148us/step - loss: 0.0650 - acc: 0.9802\n",
            "Epoch 4/10\n",
            "42000/42000 [==============================] - 6s 147us/step - loss: 0.0575 - acc: 0.9833\n",
            "Epoch 5/10\n",
            "42000/42000 [==============================] - 6s 146us/step - loss: 0.0505 - acc: 0.9854\n",
            "Epoch 6/10\n",
            "42000/42000 [==============================] - 6s 146us/step - loss: 0.0478 - acc: 0.9860\n",
            "Epoch 7/10\n",
            "42000/42000 [==============================] - 6s 146us/step - loss: 0.0457 - acc: 0.9860\n",
            "Epoch 8/10\n",
            "42000/42000 [==============================] - 6s 147us/step - loss: 0.0434 - acc: 0.9880\n",
            "Epoch 9/10\n",
            "42000/42000 [==============================] - 6s 145us/step - loss: 0.0447 - acc: 0.9866\n",
            "Epoch 10/10\n",
            "42000/42000 [==============================] - 6s 144us/step - loss: 0.0419 - acc: 0.9875\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe72a8d5d30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "SqQc1PuVIuYU",
        "colab_type": "code",
        "outputId": "3eea92f2-e548-475c-8e69-9172cd09f423",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape)\n",
        "print(X1_test.shape)\n",
        "print(y_train.shape)\n",
        "print(y1_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(42000, 28, 28, 1)\n",
            "(5940, 28, 28, 1)\n",
            "(42000, 10)\n",
            "(5940, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7tlfcqY9vJ0k",
        "colab_type": "code",
        "outputId": "788505e3-e324-4755-afa9-56c1c53cb654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model2.evaluate(X1_test, y1_test, verbose=0)\n",
        "print(scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.05488636649267823, 0.9855218855218856]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5sU0ltrXLoKF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3) Changing the parameters for the CNN in Part 3 doesn't show much difference in the testing and training accuracy. The problem and the dataset are such that a neural network can easily model itself into a almost perfect predictor. The accuracy ranges from around 97 to 99, depending on the number of layers with more layers giving better results.**"
      ]
    },
    {
      "metadata": {
        "id": "EECx0AYDSzxJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**4) The VAE model with the added Dense Layer gives us a testing accuracy of 94% while the simple CNN model gives us 98%. Although the loss in the VAE stands significantly higher at 22%, compared to the CNN loss of 5%. I believe that a simple CNN model should be preferred to a VAE in a case like this because it is a simple image classification problem without much complexity in features. The entire point of encoding images into lower dimensions and then expanding them is to learn to retain important features. However with simple digits, this in my opinion is needlessly complicating the task.\n",
        "I found that using more number of CNN in the encoder and decoder is useful and we are working with images and they are good at modeling the latent representation.**"
      ]
    }
  ]
}